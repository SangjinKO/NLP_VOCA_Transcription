{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_util import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define IEMOCAP path\n",
    "IEMOCAP_PATH = ''\n",
    "\n",
    "# define functions for file management\n",
    "def file_search(dirname, ret, list_avoid_dir=[]):\n",
    "    filenames = os.listdir(dirname)  \n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        if os.path.isdir(full_filename) :\n",
    "            if full_filename.split('/')[-1] in list_avoid_dir:\n",
    "                continue\n",
    "            else:\n",
    "                file_search(full_filename, ret, list_avoid_dir)           \n",
    "        else:\n",
    "            ret.append( full_filename )          \n",
    "\n",
    "def find_encoding(filename):\n",
    "    rawdata = open(filename, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding']    \n",
    "    return charenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for extracting transcripttion\n",
    "def extract_trans( list_in_file, out_file ) :\n",
    "    lines = []  \n",
    "    for in_file in list_in_file:\n",
    "        cnt = 0\n",
    "        with open(in_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        with open(out_file, 'a') as f:\n",
    "            csv_writer = csv.writer( f )\n",
    "            lines = sorted(lines)                  \n",
    "            for line in lines:\n",
    "                name = line.split(':')[0].split(' ')[0].strip()\n",
    "                # remove unwanted cases (noises, umatched pair in label)\n",
    "                if name[:3] != 'Ses':           \n",
    "                    continue\n",
    "                elif name[-3:-1] == 'XX':       \n",
    "                    continue\n",
    "                trans = line.split(':')[1].strip()\n",
    "                cnt += 1\n",
    "                csv_writer.writerow([name, trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transcriptions\n",
    "out_file = 'processed_transcription.csv'\n",
    "os.system('rm' + out_file)\n",
    "\n",
    "list_files = []\n",
    "\n",
    "for x in xrange(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "    path = IEMOCAP_PATH + sess_name + '/dialog/transcriptions/'\n",
    "    # sample: Ses01F_impro01_F000 [006.2901-008.2357]: Excuse me.\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "    print sess_name + \", #sum files: \" + str(len(list_files))\n",
    "\n",
    "extract_trans(list_files, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for labeling\n",
    "def find_category(lines):\n",
    "    is_target = True  \n",
    "    id = ''\n",
    "    c_label = ''\n",
    "    list_ret = []\n",
    "    for line in lines:\n",
    "        if is_target == True:\n",
    "            try:\n",
    "                id  = line.split('\\t')[1].strip()  \n",
    "                c_label  = line.split('\\t')[2].strip()  \n",
    "                if not category.has_key(c_label):\n",
    "                    print \"No key: \" + c_label\n",
    "                    sys.exit()\n",
    "                list_ret.append( [id, c_label] )\n",
    "                is_target = False\n",
    "            except:\n",
    "                print \"ERROR \" + line\n",
    "                sys.exit()\n",
    "        else:\n",
    "            if line == '\\n':\n",
    "                is_target = True\n",
    "    return list_ret\n",
    "\n",
    "def extract_labels( list_in_file, out_file ) :\n",
    "    id = ''\n",
    "    lines = []\n",
    "    list_ret = []\n",
    "    for in_file in list_in_file:\n",
    "        with open(in_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = lines[2:]                           \n",
    "            list_ret = find_category(lines)\n",
    "        list_ret = sorted(list_ret)                   \n",
    "        with open(out_file, 'a') as f:\n",
    "            csv_writer = csv.writer( f )\n",
    "            csv_writer.writerows( list_ret )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label\n",
    "out_file = 'label.csv'\n",
    "os.system('rm' + out_file)\n",
    "\n",
    "category = {'ang': 0, 'hap': 1, 'sad': 2, 'neu': 3, 'fru': 4, 'exc': 5, 'fea': 6,  'sur': 7, 'dis': 8, 'oth': 9, 'xxx': 10}\n",
    "\n",
    "list_files = []\n",
    "list_avoid_dir = ['Attribute', 'Categorical', 'Self-evaluation']\n",
    "\n",
    "for x in xrange(5):\n",
    "    sess_name = 'Session' + str(x+1)\n",
    "    path = IEMOCAP_PATH + sess_name + '/dialog/EmoEvaluation/'\n",
    "    file_search(path, list_files, list_avoid_dir)\n",
    "    list_files = sorted(list_files)\n",
    "    print sess_name + \", #sum files: \" + str(len(list_files))\n",
    "\n",
    "extract_labels(list_files, out_file)\n",
    "\n",
    "lines = []\n",
    "with open(out_file, 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    lines = [x for x in csv_reader]\n",
    "    \n",
    "new_out_file = 'new_label.csv'\n",
    "os.system('rm' + new_out_file)\n",
    "with open(new_out_file, 'a') as f:\n",
    "    csv_writer = csv.writer( f )\n",
    "    for line in lines:\n",
    "        if line[1] == 'ang': csv_writer.writerow (['0'])\n",
    "        elif line[1] == 'hap': csv_writer.writerow (['1'])\n",
    "        elif line[1] == 'exc': csv_writer.writerow (['1'])\n",
    "        elif line[1] == 'sad': csv_writer.writerow (['2'])\n",
    "        elif line[1] == 'neu': csv_writer.writerow (['3'])\n",
    "        else : csv_writer.writerow (['-1'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(new_out_file, 'r') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    lines = [x for x in csv_reader]\n",
    "\n",
    "print len([x for x in lines if x==['0']])\n",
    "print len([x for x in lines if x==['1']])\n",
    "print len([x for x in lines if x==['2']])\n",
    "print len([x for x in lines if x==['3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify \n",
    "in_file1 = 'label.csv'\n",
    "in_file2 = 'processed_transcription.csv'\n",
    "in_file3 = 'new_label.csv'\n",
    "\n",
    "label = []\n",
    "trans = []\n",
    "with open( in_file1, 'r') as f:\n",
    "    csv_reader = csv.reader( f )\n",
    "    label = [x for x in csv_reader]   \n",
    "label_id = [x[0] for x in label]\n",
    "label_var = [x[1] for x in label]\n",
    "\n",
    "with open( in_file2, 'r') as f:\n",
    "    csv_reader = csv.reader( f )\n",
    "    tran = [x for x in csv_reader]   \n",
    "tran_id = [x[0] for x in tran]\n",
    "\n",
    "with open( in_file3, 'r') as f:\n",
    "    csv_reader = csv.reader( f )\n",
    "    new_label = [x for x in csv_reader]   \n",
    "new_label_id = [x[0] for x in new_label]\n",
    "\n",
    "chk_miss = False\n",
    "for l, t in zip(label_id, tran_id):\n",
    "    if l != t:\n",
    "        print 'ERROR'\n",
    "        chk_miss = True\n",
    "\n",
    "if not(chk_miss): print(\"NO INCORRECT DATA\")\n",
    "print(\"COUNT: \", len(label_id), len(tran_id), len(new_label_id))\n",
    "\n",
    "count = np.zeros (len(category), dtype=np.int)\n",
    "for l in label_var:\n",
    "    count[category[l]] += 1\n",
    "\n",
    "for k in category:\n",
    "    print (k+' : '+str(count[category[k]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create voca from transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for NLP\n",
    "def apply_mincut(dic, min_freq):\n",
    "    print 'apply minCut and re-generate minCutDic'\n",
    "    mincut_dic = dict(filter(lambda (a, b) : b > min_freq, dic.items()))\n",
    "    print 'minFreq = ' + str(min_freq)\n",
    "    print 'original dic size = ' + str(len(dic))\n",
    "    print 'original dic word freq = ' + str(sum(dic.values()))\n",
    "    print 'minCut dic size = ' + str(len(mincut_dic))\n",
    "    print 'minCut dic word freq = ' + str(sum(mincut_dic.values()))\n",
    "    coverage = sum(mincut_dic.values()) / float(sum(dic.values()))\n",
    "    print 'coverage = ' + str(coverage)\n",
    "    return mincut_dic\n",
    "\n",
    "# value <-> key\n",
    "def create_invert_dic( dic ) :\n",
    "    inv_dic = {}\n",
    "    for key in dic.keys() :\n",
    "        inv_dic[ dic[key] ] = key\n",
    "    \n",
    "    return inv_dic\n",
    "\n",
    "# print sentences\n",
    "def index_to_sentence( inv_dic, list_index ) :\n",
    "    print [ inv_dic[ x ] for x in list_index ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voca and check\n",
    "lines = []\n",
    "with open('processed_transcription.csv') as f:\n",
    "    read = csv.reader(f)\n",
    "    lines = [ x[1] for x in read]\n",
    "    \n",
    "token_lines = [word_tokenize(x) for x in lines]\n",
    "#word_tokenize: param text:\ttext to split into sentences\n",
    "#e.g.: ['Excuse', 'me', '.']\n",
    "token_lines_lower = [ [t.lower() for t in x] for x in token_lines]\n",
    "sent_len = [len(x) for x in token_lines]\n",
    "\n",
    "print 'max : ' + str(np.max(sent_len))\n",
    "print 'min : ' + str(np.min(sent_len))\n",
    "print 'mean : ' + str(np.mean(sent_len))\n",
    "print 'std : ' + str(np.std(sent_len))\n",
    "\n",
    "dic_count = {}\n",
    "for tokens in token_lines_lower:\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token in dic_count :\n",
    "            dic_count[token] += 1\n",
    "        else:\n",
    "            dic_count[token] = 1\n",
    "print 'dic size : ' + str(len(dic_count))\n",
    "\n",
    "# dic_count = apply_mincut(dic_count, 1)\n",
    "# print 'dic size : ' + str(len(dic_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "dic['_PAD_'] = 0\n",
    "dic['_UNK_'] = 0\n",
    "\n",
    "for word in dic_count.keys():\n",
    "    dic[word] = len(dic)    \n",
    "print len(dic)\n",
    "print dic['_PAD_']\n",
    "print dic['_UNK_']\n",
    "\n",
    "with open('dic.pkl', 'w') as f:\n",
    "    pickle.dump( dic, f )\n",
    "    \n",
    "# convert to index\n",
    "index_lines = [ [ dic[t] for t in x ] for x in token_lines_lower ]\n",
    "\n",
    "len_max = np.max(sent_len)\n",
    "np_trans = np.zeros( [len(index_lines), len_max], dtype=np.int)\n",
    "for i in range(len(index_lines)):\n",
    "    if len( index_lines[i] ) >= len_max:\n",
    "        np_trans[i][:] = index_lines[i][:len_max]\n",
    "    else:\n",
    "        np_trans[i][:len(index_lines[i])] = index_lines[i][:]\n",
    "\n",
    "print(np.shape(np_trans))\n",
    "np.save('processed_transcription.npy', np_trans)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create word embedding (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for creating word embedding\n",
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = codecs.open(gloveFile,'r')\n",
    "    \n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "def cal_coverage(voca, glove):\n",
    "    cnt = 0\n",
    "    for token in voca.keys():\n",
    "        if glove.has_key( token ):\n",
    "            ;\n",
    "        else:\n",
    "            cnt = cnt + 1  \n",
    "\n",
    "    print '# missing token : ' + str(cnt)\n",
    "    print 'coverage : ' + str( 1 - ( cnt/ float(len(voca)) ) ) \n",
    "    \n",
    "def create_glove_embedding(voca, glove):\n",
    "\n",
    "    # sorting voca dic by value\n",
    "    sorted_voca = sorted(voca.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    list_glove_voca = []\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for token, value in sorted_voca:\n",
    "\n",
    "        if glove.has_key( token ):\n",
    "            list_glove_voca.append( glove[token] )\n",
    "        else:\n",
    "            if token == '_PAD_':\n",
    "                print 'add PAD as 0s'\n",
    "                list_glove_voca.append( np.zeros(300) )  \n",
    "            else:\n",
    "                list_glove_voca.append( np.random.uniform(-0.25, 0.25, 300).tolist() )\n",
    "                cnt = cnt + 1  \n",
    "\n",
    "    print 'coverage : ' + str( 1 - ( cnt/ float(len(voca)) ) )\n",
    "    return list_glove_voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load GloVe model\n",
    "print 'loading started'\n",
    "# Ref: download 'Common Crawl' from 'https://nlp.stanford.edu/projects/glove/'\n",
    "glove = loadGloveModel('E:\\MyGithub\\GloVe_RAW_DATA\\glove.840B.300d.txt')\n",
    "print 'loading completed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create & store word embedding\n",
    "dic = {}\n",
    "with open('dic.pkl') as f:\n",
    "    dic = pickle.load(f)\n",
    "print 'total dic size : ' + str(len(dic))\n",
    "\n",
    "cal_coverage(dic, glove)\n",
    "\n",
    "list_glove_voca = create_glove_embedding(dic, glove)\n",
    "len(list_glove_voca)\n",
    "\n",
    "np_glove = np.asarray(list_glove_voca, dtype=np.float32)\n",
    "print np.shape(np_glove)\n",
    "np.save('word_embedding.npy', np_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
